{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368aa7f5",
   "metadata": {},
   "source": [
    "## Sound classification techniques_functions from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370b48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0fa9f",
   "metadata": {},
   "source": [
    "A function that implements a linear model function for binary logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483701f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_function(data_matrix, weights):\n",
    "    P=data_matrix\n",
    "    W=weights\n",
    "    return P@W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27862bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49035014],\n",
       "       [-2.0367235 ],\n",
       "       [ 0.73864952]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_matrix = np.array([[1, -0.15771414, -0.75293918, -0.0130713],\\\n",
    "                             [1, -0.3521762, 0.19650084, 1.94050566],\\\n",
    "                             [1, 1.30917135,  1.69791787, -0.91502836]])\n",
    "test_weights = np.array([[0.46390948], [-1.15729779], [0.23372497],\n",
    "                         [-1.5223521]])\n",
    "\n",
    "linear_model_function(test_data_matrix, test_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90d8c9",
   "metadata": {},
   "source": [
    "Function that takes an argument named inputs and returns the output of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7e1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_logistic_activation_function(inputs):\n",
    "    di=1+np.exp(-inputs)\n",
    "    return 1/di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b301c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.73105858, 0.04742587, 0.66666667, 0.6040482 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_row_inputs = np.array([0, 1, -3, np.log(2), 0.4223615])\n",
    "binary_logistic_activation_function(test_row_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66614097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11920292],\n",
       "       [0.62245933],\n",
       "       [0.25      ],\n",
       "       [0.56581268]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_column_inputs = np.array([[-2], [0.5], [np.log(1 / 3)], [0.26478703]])\n",
    "binary_logistic_activation_function(test_column_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba2109",
   "metadata": {},
   "source": [
    "Function takes the argument logistic_values as inputs and returns a vector of class labels with binary values in  {0,1} as its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efdfe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_logistic_prediction_function(logistic_values):\n",
    "    logistic_values[logistic_values < 0.5] = 0\n",
    "    logistic_values[logistic_values > 0.5] = 1\n",
    "    return logistic_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecb4087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logistic_values = np.array([[0.30493025], [0.80419595], [0.56509748], [0.23903961],\\\n",
    "                                 [0.9773376],  [0.28956517], [0.83508464], [0.57761601],\\\n",
    "                                 [0.41169655], [0.97455507], [0.1095421],  [0.21852319]])\n",
    "\n",
    "binary_logistic_prediction_function(test_logistic_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc6eeb",
   "metadata": {},
   "source": [
    "Function that takes two inputs true_labels and recovered_labels and returns the percentage of correctly classified labels divided by  100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf99b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(true_labels, recovered_labels):\n",
    "    equal_labels = recovered_labels == true_labels\n",
    "    return np.mean(equal_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd118c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_true_labels = np.array([[0], [1], [1], [0], [1], [1], [0], [0], [1], [0]])\n",
    "test_recovered_labels = np.array([[1], [1], [0], [0], [0], [1], [1], [0], [1],\n",
    "                                  [1]])\n",
    "\n",
    "classification_accuracy(test_true_labels, test_recovered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cb7b7",
   "metadata": {},
   "source": [
    "Write two functions that implement the cost function for binary logistic regression as well as its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26140dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_logistic_regression_cost_function(data_matrix, data_labels,\n",
    "                                             weights):\n",
    "    B_out = linear_model_function(data_matrix, weights)\n",
    "    return np.mean(np.log(1 + np.exp(B_out)) - data_labels * B_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25bd978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_logistic_regression_gradient(data_matrix, data_labels, weights):\n",
    "    B_out = linear_model_function(data_matrix, weights)\n",
    "    P=data_matrix\n",
    "    s=len(data_matrix)\n",
    "    return P.T @ (binary_logistic_activation_function(B_out) - data_labels) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc46ae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.846796279379529"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_matrix = np.array([[1, -0.12793802, -0.19751682, 0.15226261], [1, -0.2000033, 0.52618148, 0.8782188],\\\n",
    "                             [1, -0.8613664, 0.30565866, -0.49296481], [1, -0.84047233, 0.9252299, 0.99071071],\\\n",
    "                             [1, -0.1042736, 0.52956168, -0.47798761], [1, 0.11284415, -0.94223132, 0.65812316],\\\n",
    "                             [1, -0.24878785, 0.85320211, -0.20377839], [1, 0.86694977, -0.55784702, 0.39560232],\\\n",
    "                             [1, 0.63581885, -0.11724269, -0.0979702], [1, 0.92166492, 0.76064856, 0.20755241]])\n",
    "test_data_labels = np.array([[0], [1], [1], [1], [0], [1], [0], [0], [1], [0]])\n",
    "test_weights = np.array([[-0.10541393], [0.57801403], [0.67163957],\n",
    "                         [-0.34652194]])\n",
    "\n",
    "binary_logistic_regression_cost_function(data_matrix=test_data_matrix,\n",
    "                                             data_labels=test_data_labels,\n",
    "                                             weights=test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3aafa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.006401  ],\n",
       "       [ 0.15045298],\n",
       "       [ 0.07449562],\n",
       "       [-0.12069674]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_matrix = np.array([[1, -0.12793802, -0.19751682, 0.15226261], [1, -0.2000033, 0.52618148, 0.8782188],\\\n",
    "                             [1, -0.8613664, 0.30565866, -0.49296481], [1, -0.84047233, 0.9252299, 0.99071071],\\\n",
    "                             [1, -0.1042736, 0.52956168, -0.47798761], [1, 0.11284415, -0.94223132, 0.65812316],\\\n",
    "                             [1, -0.24878785, 0.85320211, -0.20377839], [1, 0.86694977, -0.55784702, 0.39560232],\\\n",
    "                             [1, 0.63581885, -0.11724269, -0.0979702], [1, 0.92166492, 0.76064856, 0.20755241]])\n",
    "test_data_labels = np.array([[0], [1], [1], [1], [0], [1], [0], [0], [1], [0]])\n",
    "test_weights = np.array([[-0.10541393], [0.57801403], [0.67163957],\n",
    "                         [-0.34652194]])\n",
    "\n",
    "binary_logistic_regression_gradient(data_matrix=test_data_matrix,\n",
    "                                        data_labels=test_data_labels,\n",
    "                                        weights=test_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc898c58",
   "metadata": {},
   "source": [
    " Implementation of a gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4c052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(objective,\n",
    "                     gradient,\n",
    "                     initial_weights,\n",
    "                     step_size=1,\n",
    "                     no_of_iterations=100,\n",
    "                     print_output=10):\n",
    "    obj = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    N=no_of_iterations\n",
    "    obj.append(objective(weights))\n",
    "    \n",
    "    for i in range(N):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        obj.append(objective(weights))\n",
    "        \n",
    "        if (i + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=i+1, m=N, o=obj[i]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=i+ 1, m=N, o=obj[i]))\n",
    "    \n",
    "    return weights, obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ab795",
   "metadata": {},
   "source": [
    "gradient_descent_v2 takes an additional argument tolerance that controls the norm of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "954f21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_v2(objective, gradient, initial_weights, \\\n",
    "                        step_size=1, no_of_iterations=100, print_output=10, tolerance=1e-6):\n",
    "    obj = []\n",
    "    L=objective\n",
    "    weights = np.copy(initial_weights)\n",
    "    N=no_of_iterations\n",
    "    obj.append(L(weights))\n",
    "    \n",
    "    for i in range(N):\n",
    "        dL=gradient(weights)\n",
    "        if np.linalg.norm(dL,2) <= tolerance:\n",
    "            break\n",
    "        weights -= step_size * dL\n",
    "        obj.append(L(weights))\n",
    "        \n",
    "        if (i + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=i+1, m=N, o=obj[i]))\n",
    "        \n",
    "        \n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=i+1, m=N, o=obj[i]))\n",
    "    \n",
    "    return weights, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0388024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/100, objective = -3.0256011293288547.\n",
      "Iteration 20/100, objective = -3.0256410067129824.\n",
      "Iteration 30/100, objective = -3.025641025632046.\n",
      "Iteration 40/100, objective = -3.0256410256410207.\n",
      "Iteration 50/100, objective = -3.025641025641026.\n",
      "Iteration 60/100, objective = -3.0256410256410255.\n",
      "Iteration 70/100, objective = -3.0256410256410264.\n",
      "Iteration 80/100, objective = -3.0256410256410255.\n",
      "Iteration 90/100, objective = -3.0256410256410255.\n",
      "Iteration 100/100, objective = -3.0256410256410255.\n",
      "Iteration completed after 100/100, objective = -3.0256410256410255.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.56410256, -0.53846154])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix_m = np.array([[3, 1], [2, 4]])\n",
    "test_vector_v = np.array([5, 6])\n",
    "test_objective = lambda x: x.T @ (test_matrix_m @ x) + x @ test_vector_v\n",
    "test_gradient = lambda x: (test_matrix_m + test_matrix_m.T) @ x + test_vector_v\n",
    "test_initial_weights = np.array([0.0, 0.0])\n",
    "test_step_size = 0.9 / (np.linalg.norm(test_matrix_m + test_matrix_m.T))\n",
    "test_no_of_iterations = 100\n",
    "test_print_output = 10\n",
    "\n",
    "gradient_descent(test_objective, test_gradient, \\\n",
    "                                           test_initial_weights,test_step_size, \\\n",
    "                                           test_no_of_iterations, test_print_output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58d48595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/100, objective = -3.0256011293288547.\n",
      "Iteration 20/100, objective = -3.0256410067129824.\n",
      "Iteration 30/100, objective = -3.025641025632046.\n",
      "Iteration completed after 36/100, objective = -3.025641025640935.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix_m = np.array([[3, 1], [2, 4]])\n",
    "test_vector_v = np.array([5, 6])\n",
    "test_objective = lambda x: x.T @ (test_matrix_m @ x) + x @ test_vector_v\n",
    "test_gradient = lambda x: (test_matrix_m + test_matrix_m.T) @ x + test_vector_v\n",
    "test_initial_weights = np.array([0.0, 0.0])\n",
    "test_step_size = 0.9 / (np.linalg.norm(test_matrix_m + test_matrix_m.T))\n",
    "test_no_of_iterations = 100\n",
    "test_print_output = 10\n",
    "\n",
    "len(gradient_descent_v2(test_objective, test_gradient, \\\n",
    "                                           test_initial_weights,test_step_size, \\\n",
    "                                           test_no_of_iterations, test_print_output)[1])-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111a9cb",
   "metadata": {},
   "source": [
    "Implementation of the functions on the Spotify music dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2eb762",
   "metadata": {},
   "source": [
    "To load and store training and testing dataset into corresponding NumPy arrays: spotify_training_data_input, spotify_training_data_labels, spotify_testing_data_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e358645",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_training_data = np.genfromtxt('spotify_training.csv',\n",
    "                                      skip_header=True,\n",
    "                                      dtype=None,\n",
    "                                      delimiter=',')\n",
    "spotify_testing_data_input = np.genfromtxt('spotify_testing.csv',\n",
    "                                           skip_header=True,\n",
    "                                           dtype=None,\n",
    "                                           delimiter=',')\n",
    "spotify_training_data_input = spotify_training_data[:, :-1]\n",
    "spotify_training_data_labels = spotify_training_data[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1d880",
   "metadata": {},
   "source": [
    "A function to standardise the columns of a two-dimensional NumPy array data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1929315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    P=data_matrix\n",
    "    row_means = np.mean(data_matrix, axis=0)\n",
    "    std_matrix = P - row_means\n",
    "    row_stds = np.std(std_matrix, axis=0)\n",
    "    return (std_matrix / row_stds), row_means, row_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2991411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_matrix = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "test_standardised_matrix,test_row_of_means,test_row_of_stds = standardise(test_data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148b423",
   "metadata": {},
   "source": [
    "The function that de-standardises the columns of a two-dimensional NumPy array data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db24d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_standardise(standardised_matrix, row_of_means, row_of_stds):\n",
    "    matrix = np.copy(standardised_matrix * row_of_stds)\n",
    "    return matrix + row_of_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5355c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.],\n",
       "       [5., 6.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_standardise(test_standardised_matrix, test_row_of_means,\n",
    "                   test_row_of_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d0a57",
   "metadata": {},
   "source": [
    "Apply data standardisation as per above to the Spotify data, both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "629774db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_training_data_input, spotify_row_of_avgs, spotify_row_of_stds = standardise(\n",
    "    spotify_training_data_input)\n",
    "spotify_testing_data_input = (spotify_testing_data_input -\n",
    "                              spotify_row_of_avgs) / spotify_row_of_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad116499",
   "metadata": {},
   "source": [
    " Implement function linear_regression_data that computes (and outputs) the linear regression data_matrix for a given data_inputs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cf12cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_data(data_inputs):\n",
    "    col=np.ones((len(data_inputs), 1))\n",
    "    mat=np.c_[col,data_inputs]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac46097e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 3., 4.],\n",
       "       [1., 4., 5.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_inputs = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "linear_regression_data(test_data_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf431a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/1000, objective = 0.22372733676269302.\n",
      "Iteration 200/1000, objective = 0.20311361947131654.\n",
      "Iteration 300/1000, objective = 0.19292482679017547.\n",
      "Iteration 400/1000, objective = 0.18637072867302748.\n",
      "Iteration 500/1000, objective = 0.1817524285331569.\n",
      "Iteration 600/1000, objective = 0.1783355175541878.\n",
      "Iteration completed after 624/1000, objective = 0.17764570891883327.\n"
     ]
    }
   ],
   "source": [
    "spotify_training_data_matrix = linear_regression_data(spotify_training_data_input) \n",
    "spotify_objective= lambda weights: binary_logistic_regression_cost_function(spotify_training_data_matrix,\\\n",
    "                                                                    spotify_training_data_labels, weights)\n",
    "spotify_gradient= lambda weights: binary_logistic_regression_gradient(spotify_training_data_matrix,\\\n",
    "                                                                    spotify_training_data_labels, weights)\n",
    "\n",
    "spotify_initial_weights=np.zeros((len(spotify_training_data_matrix.T), 1))\n",
    "\n",
    "s=len(spotify_training_data_matrix)\n",
    "\n",
    "spotify_step_size=(3.9*s)/(np.linalg.norm(spotify_training_data_matrix))**2\n",
    "\n",
    "spotify_optimal_weights,spotify_objective_values=gradient_descent_v2(spotify_objective, spotify_gradient, \\\n",
    "                                                        spotify_initial_weights,spotify_step_size, 1000, 100, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f59266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.66666666666666\n"
     ]
    }
   ],
   "source": [
    "Labels=binary_logistic_prediction_function(linear_model_function(spotify_training_data_matrix, spotify_optimal_weights))\n",
    "spotify_classification_accuracy=classification_accuracy(Labels, spotify_training_data_labels)\n",
    "print(spotify_classification_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5a887",
   "metadata": {},
   "source": [
    "To increase the classification accuracy by considering the ridge binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f5aa2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_binary_logistic_regression_cost_function(data_matrix, data_labels,\n",
    "                                                   weights,\n",
    "                                                   regularisation_parameter):\n",
    "    L=binary_logistic_regression_cost_function(data_matrix, data_labels,weights)\n",
    "    a=regularisation_parameter\n",
    "    w=weights\n",
    "    return L+(a/2)*np.linalg.norm(w)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34b6b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_binary_logistic_regression_gradient(data_matrix, data_labels,\n",
    "                                              weights,\n",
    "                                              regularisation_parameter):\n",
    "    dL=binary_logistic_regression_gradient(data_matrix, data_labels,weights)\n",
    "    a=regularisation_parameter\n",
    "    w=weights\n",
    "    return dL+a*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3955193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3049910205825461"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_matrix = np.array([[1, -0.12793802, -0.19751682, 0.15226261], [1, -0.2000033, 0.52618148, 0.8782188],\\\n",
    "                             [1, -0.8613664, 0.30565866, -0.49296481], [1, -0.84047233, 0.9252299, 0.99071071],\\\n",
    "                             [1, -0.1042736, 0.52956168, -0.47798761], [1, 0.11284415, -0.94223132, 0.65812316],\\\n",
    "                             [1, -0.24878785, 0.85320211, -0.20377839], [1, 0.86694977, -0.55784702, 0.39560232],\\\n",
    "                             [1, 0.63581885, -0.11724269, -0.0979702], [1, 0.92166492, 0.76064856, 0.20755241]])\n",
    "test_data_labels = np.array([[0], [1], [1], [1], [0], [1], [0], [0], [1], [0]])\n",
    "test_weights = np.array([[-0.10541393], [0.57801403], [0.67163957],\n",
    "                         [-0.34652194]])\n",
    "regularisation_parameter = 1\n",
    "\n",
    "ridge_binary_logistic_regression_cost_function(\n",
    "        data_matrix=test_data_matrix,\n",
    "        data_labels=test_data_labels,\n",
    "        weights=test_weights,\n",
    "        regularisation_parameter=regularisation_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee902e2e",
   "metadata": {},
   "source": [
    "Write a function grid_search that performs a search for a minimum value of a given function on a given grid points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8366c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(objective, grid):\n",
    "    values = np.array([])\n",
    "    for point in grid:\n",
    "        values = np.append(values, objective(point))\n",
    "    return grid[np.argmin(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f08318a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_objective = lambda x: x[0]**2 - 2 * x[0] * x[1] + 2 * x[1]**2 + x[\n",
    "    0] - 3 * x[1]\n",
    "test_grid = [(x, y) for x in range(5) for y in range(5)]\n",
    "\n",
    "grid_search(test_objective, test_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f9421",
   "metadata": {},
   "source": [
    "To find the optimal value of a hyperparameter regularisation_parameter and then find the spotify_optimal_weights corresponding to the value of the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cad252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration completed after 2000/2000, objective = 0.16459394936632762.\n",
      "Iteration completed after 1637/2000, objective = 0.24916613118975584.\n",
      "Iteration completed after 953/2000, objective = 0.2812102450711464.\n",
      "Iteration completed after 677/2000, objective = 0.30325224596275335.\n",
      "Iteration completed after 526/2000, objective = 0.3204602727013894.\n",
      "Iteration completed after 431/2000, objective = 0.3347162465423249.\n",
      "Iteration completed after 366/2000, objective = 0.34694808475640804.\n",
      "Iteration completed after 318/2000, objective = 0.3576911694901299.\n",
      "Iteration completed after 282/2000, objective = 0.36728610418910934.\n",
      "Iteration completed after 253/2000, objective = 0.37596462132071534.\n",
      "Iteration completed after 859/2000, objective = 0.3032522459685494.\n",
      "The classification accuracy for the training set is 92.67 %. This is achieved for the hyperparameter value 0.03\n"
     ]
    }
   ],
   "source": [
    "spotify_regularisation_parameter_grid = np.arange(0, .1, 0.01)\n",
    "\n",
    "spotify_validation_error = lambda regularisation_parameter: 1-classification_accuracy(spotify_training_data_labels,\\\n",
    "                    binary_logistic_prediction_function(\n",
    "                    binary_logistic_activation_function(\n",
    "                    linear_model_function(spotify_training_data_matrix,\n",
    "                    gradient_descent_v2(\n",
    "                    objective = lambda weights: ridge_binary_logistic_regression_cost_function(\n",
    "                                spotify_training_data_matrix,\n",
    "                                spotify_training_data_labels,\n",
    "                                weights,\n",
    "                                regularisation_parameter),\n",
    "                    gradient = lambda weights: ridge_binary_logistic_regression_gradient(\n",
    "                                spotify_training_data_matrix,\n",
    "                                spotify_training_data_labels,\n",
    "                                weights,\n",
    "                                regularisation_parameter),\n",
    "                    initial_weights = np.zeros(shape = (spotify_training_data_matrix.shape[1], 1)),\n",
    "                    step_size = 1/(np.linalg.norm(spotify_training_data_matrix)**2/(3.9*len(spotify_training_data_matrix))+regularisation_parameter),\n",
    "                    no_of_iterations = 2000,\n",
    "                    print_output = 2001,\n",
    "                    tolerance = 1e-5\n",
    "                    )[0]))))\n",
    "\n",
    "spotify_optimal_regularisation_parameter = grid_search(\n",
    "    spotify_validation_error, spotify_regularisation_parameter_grid)\n",
    "\n",
    "spotify_optimal_weights = gradient_descent_v2(\n",
    "    objective=lambda weights: ridge_binary_logistic_regression_cost_function(\n",
    "        spotify_training_data_matrix, spotify_training_data_labels, weights,\n",
    "        spotify_optimal_regularisation_parameter),\n",
    "    gradient=lambda weights: ridge_binary_logistic_regression_gradient(\n",
    "        spotify_training_data_matrix, spotify_training_data_labels, weights,\n",
    "        spotify_optimal_regularisation_parameter),\n",
    "    initial_weights=np.zeros(shape=(spotify_training_data_matrix.shape[1], 1)),\n",
    "    step_size=1 /\n",
    "    (np.linalg.norm(spotify_training_data_matrix)**2 /\n",
    "     (3.9 * len(spotify_training_data_matrix)) + regularisation_parameter),\n",
    "    no_of_iterations=2000,\n",
    "    print_output=2001,\n",
    "    tolerance=1e-5)[0]\n",
    "\n",
    "spotify_training_regression_values = linear_model_function(\n",
    "    spotify_training_data_matrix, spotify_optimal_weights)\n",
    "spotify_training_predicted_labels = binary_logistic_prediction_function(\n",
    "    binary_logistic_activation_function(spotify_training_regression_values))\n",
    "spotify_classification_accuracy = classification_accuracy(true_labels=spotify_training_data_labels,\\\n",
    "                                                         recovered_labels=spotify_training_predicted_labels)\n",
    "print(\n",
    "    \"The classification accuracy for the training set is {p:.2f} %. This is achieved for the hyperparameter value {a}\"\n",
    "    .format(p=100 * spotify_classification_accuracy,\n",
    "            a=spotify_optimal_regularisation_parameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf495c2",
   "metadata": {},
   "source": [
    "## Music genre classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f62d2e",
   "metadata": {},
   "source": [
    "To implement softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3abe8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(argument, axis=None):\n",
    "    if axis == None:\n",
    "        output = np.exp(argument - np.max(argument))\n",
    "        output = output / np.sum(output)\n",
    "    else:\n",
    "        output = np.exp(argument - np.expand_dims(np.max(argument, axis), axis))\n",
    "        output = output / np.expand_dims(np.sum(output, axis), axis)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "783bf31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76528029],\n",
       "       [0.23049799],\n",
       "       [0.00422172]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_function(np.array([[1.5], [0.3], [-3.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b92510",
   "metadata": {},
   "source": [
    "To write a function multinomial_prediction_function that turns your predictions into labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e411dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_prediction_function(data_matrix, weight_matrix):\n",
    "    Mod=linear_model_function(data_matrix, weight_matrix)\n",
    "    return np.argmax(Mod, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a07a1",
   "metadata": {},
   "source": [
    "write a function one_hot_vector_encoding that converts an NumPy array labels with values in the range of {0,𝐾−1} into so-called one-hot vector encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f598505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector_encoding(labels):\n",
    "    N = np.max(labels) + 1\n",
    "    output = np.zeros((len(labels), N))\n",
    "    output[np.arange(len(labels)), labels] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75184934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector_encoding(np.array([1, 2, 0, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e921b0",
   "metadata": {},
   "source": [
    "Implement the cost function and gradient for the multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ce93d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_cost_function(data_matrix, weight_matrix,\n",
    "                                                  one_hot_vector_encodings):\n",
    "    s=len(data_matrix)\n",
    "    mod = data_matrix @ weight_matrix\n",
    "    return (np.sum(np.log(np.sum(np.exp(mod), axis=1)) \\\n",
    "                   - np.sum(one_hot_vector_encodings * mod, axis=1)))/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb1299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_gradient(data_matrix, weight_matrix,\n",
    "                                             one_hot_vector_encodings):\n",
    "    mod = data_matrix @ weight_matrix\n",
    "    s=len(data_matrix)\n",
    "    return (data_matrix.T @(softmax_function(mod, axis=1) - one_hot_vector_encodings))/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2833033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028611028678354876"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_matrix = np.array([[6, 4, 5], [1, 2, 8], [-3, 3, 6], [6, 5, -100],\n",
    "                             [5, 7, 2]])\n",
    "test_weight_matrix = np.array([[2, 1, -2, -4], [2, -5, 1, 4], [-2, -3, -1,\n",
    "                                                               -2]])\n",
    "test_one_hot_vector_encoding = np.array([[1., 0., 0., 0.], [0., 0., 1., 0.],\n",
    "                                         [0., 0., 0., 1.], [0., 1., 0., 0.],\n",
    "                                         [1., 0., 0., 0.]])\n",
    "multinomial_logistic_regression_cost_function(\n",
    "        test_data_matrix, test_weight_matrix, test_one_hot_vector_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12181218",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTZAN_data = np.genfromtxt('GTZAN_features.csv',\n",
    "                           skip_header=True,\n",
    "                           dtype=float,\n",
    "                           delimiter=',',\n",
    "                           usecols=range(1, 59))\n",
    "GTZAN_data, _, _ = standardise(GTZAN_data)\n",
    "\n",
    "genres_dictionary = {b'blues':0,b'classical':1,b'country':2,\\\n",
    "                    b'disco':3,b'hiphop':4,b'jazz':5,\\\n",
    "                     b'metal':6,b'pop':7,b'reggae':8,b'rock':9}\n",
    "GTZAN_labels = np.genfromtxt('GTZAN_features.csv',\n",
    "                             skip_header=True,\n",
    "                             dtype=str,\n",
    "                             delimiter=',',\n",
    "                             usecols=[59],\n",
    "                             converters={59: lambda x: genres_dictionary[x]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e1a30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/1000, objective = 1.0767506389614978.\n",
      "Iteration 200/1000, objective = 0.910981484867509.\n",
      "Iteration 300/1000, objective = 0.8244841216515565.\n",
      "Iteration 400/1000, objective = 0.7683732070923374.\n",
      "Iteration 500/1000, objective = 0.7279186772726307.\n",
      "Iteration 600/1000, objective = 0.6968211831203023.\n",
      "Iteration 700/1000, objective = 0.6718623190153213.\n",
      "Iteration 800/1000, objective = 0.6512000503453517.\n",
      "Iteration 900/1000, objective = 0.6336912713100291.\n",
      "Iteration 1000/1000, objective = 0.6185821006106668.\n",
      "Iteration completed after 1000/1000, objective = 0.6185821006106668.\n"
     ]
    }
   ],
   "source": [
    "GTZAN_data_matrix = linear_regression_data(GTZAN_data) \n",
    "GTZAN_OHV = one_hot_vector_encoding(GTZAN_labels)\n",
    "GTZAN_objective= lambda weight_matrix: multinomial_logistic_regression_cost_function(GTZAN_data_matrix,\\\n",
    "                                                                        weight_matrix, GTZAN_OHV)\n",
    "GTZAN_gradient = lambda weight_matrix:multinomial_logistic_regression_gradient(GTZAN_data_matrix,\\\n",
    "                                                                        weight_matrix, GTZAN_OHV)\n",
    "\n",
    "s2=len(GTZAN_data_matrix)\n",
    "\n",
    "GTZAN_step_size = (3.9*s2)/(np.linalg.norm(GTZAN_data_matrix))**2\n",
    "GTZAN_initial_weights = np.zeros((GTZAN_data_matrix.shape[1], GTZAN_OHV.shape[1]))\n",
    "\n",
    "\n",
    "GTZAN_optimal_weights, GTZAN_objective_values=gradient_descent_v2(GTZAN_objective, GTZAN_gradient,\\\n",
    "                      GTZAN_initial_weights, GTZAN_step_size, 1000, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7e1503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.581158794776619"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(GTZAN_optimal_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "266475de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6184412193103138"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GTZAN_objective_values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "367152c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the GTZAN dataset is 81.89999999999999 %.\n"
     ]
    }
   ],
   "source": [
    "GTZAN_recovered_labels = multinomial_prediction_function(GTZAN_data_matrix, \\\n",
    "                                                         GTZAN_optimal_weights)\n",
    "GTZAN_classification_accuracy = classification_accuracy(\n",
    "    GTZAN_labels, GTZAN_recovered_labels)\n",
    "print(\"The classification accuracy for the GTZAN dataset is {p} %.\".format(\n",
    "    p=100 * GTZAN_classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb7b3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholding(argument, threshold):\n",
    "    return np.sign(argument) * np.maximum(0, np.abs(argument) - threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "caec92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_logistic_regression_cost_function(data_matrix, weight_matrix,\n",
    "                                            one_hot_vector_encodings,\n",
    "                                            regularisation_parameter):\n",
    "    MSE=multinomial_logistic_regression_cost_function(data_matrix, weight_matrix,one_hot_vector_encodings)\n",
    "    a=regularisation_parameter\n",
    "    return MSE+a * np.sum(np.abs(weight_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c15b83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_gradient_descent(objective,\n",
    "                              gradient,\n",
    "                              proximal_map,\n",
    "                              initial_weights,\n",
    "                              step_size=1,\n",
    "                              no_of_iterations=1000,\n",
    "                              print_output=100):\n",
    "    obj_list=[]\n",
    "    W=initial_weights\n",
    "    print(gradient)\n",
    "    obj_list.append(objective(W))\n",
    "    N=no_of_iterations\n",
    "    s=step_size\n",
    "    for i in range(N):\n",
    "        W=proximal_map(W-s*gradient(W))\n",
    "        obj_list.append(objective(W))\n",
    "        if (i+1)%print_output==0:\n",
    "            print(\"Iteration {k}/{m}, objective={o}.\".format(k=i+1,m=no_of_iterations, o=obj_list[i]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=i+1,m=no_of_iterations, o=obj_list[i]))\n",
    "    return W,obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4f9d254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 1, -2]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weight_matrix = np.array([[1, 2], [3, -4]])\n",
    "soft_thresholding(test_weight_matrix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b55f298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fa1508fee50>\n",
      "Iteration 100/1000, objective=1.0825816929822865.\n",
      "Iteration 200/1000, objective=0.9188065638028389.\n",
      "Iteration 300/1000, objective=0.8337438517624016.\n",
      "Iteration 400/1000, objective=0.7787614112668051.\n",
      "Iteration 500/1000, objective=0.7392524994112956.\n",
      "Iteration 600/1000, objective=0.7089861854744975.\n",
      "Iteration 700/1000, objective=0.6847639172736621.\n",
      "Iteration 800/1000, objective=0.6647871984883266.\n",
      "Iteration 900/1000, objective=0.6479213697436089.\n",
      "Iteration 1000/1000, objective=0.6334014439684763.\n",
      "Iteration completed after 1000/1000, objective = 0.6334014439684763.\n"
     ]
    }
   ],
   "source": [
    "LASSO_regularisation_parameter = 1e-4\n",
    "\n",
    "GTZAN_LASSO_objective= lambda weight_matrix: lasso_logistic_regression_cost_function(GTZAN_data_matrix,\\\n",
    "                                           weight_matrix, GTZAN_OHV,LASSO_regularisation_parameter)\n",
    "GTZAN_LASSO_gradient = lambda weight_matrix:multinomial_logistic_regression_gradient(GTZAN_data_matrix,\\\n",
    "                                                                        weight_matrix, GTZAN_OHV)\n",
    "\n",
    "k=LASSO_regularisation_parameter*GTZAN_step_size\n",
    "GTZAN_proximal_map = lambda arg:soft_thresholding(arg, k)\n",
    "\n",
    "\n",
    "GTZAN_LASSO_optimal_weights, GTZAN_LASSO_objective_values=proximal_gradient_descent(GTZAN_LASSO_objective,GTZAN_LASSO_gradient,\\\n",
    "                              GTZAN_proximal_map, GTZAN_initial_weights, GTZAN_step_size, 1000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be2920d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the GTZAN dataset is 81.89999999999999 %.\n"
     ]
    }
   ],
   "source": [
    "GTZAN_recovered_labels = multinomial_prediction_function(GTZAN_data_matrix,GTZAN_LASSO_optimal_weights)\n",
    "GTZAN_LASSO_classification_accuracy = classification_accuracy(GTZAN_labels, GTZAN_recovered_labels)\n",
    "print(\"The classification accuracy for the GTZAN dataset is {p} %.\".format(p=100 * GTZAN_classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744ebfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
